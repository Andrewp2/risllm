\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025

 \usepackage[preprint]{neurips_2025}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\title{Resampled Importance Sampling Is All You Need}

\author{
  Andrew M.~Peterson \\
 Independent Researcher \\
  \texttt{andrewpetersongamedev@gmail.com} \\
}


\begin{document}


\maketitle


\begin{abstract}
  Recent work has shown that test-time inference is an effective way to improve large language model performance across many benchmarks. However, test-time inference approaches require exponential compute for linear improvements in performance, preventing them from efficiently achieving high performance. We present Resampled Importance Sampling for Language Models (RIS), a novel method to improve efficiency of test-time inference. RIS does not require a reward model, verifiable rewards, or any retraining. Our experiments show that RIS significantly enhances language model's problem-solving abilities across a diverse range of benchmarks (GSM8K, MATH500, IFEval, MMLU-Redux, and ARC-AGI 2) for compute-equivalent configurations. We provide an open-source Github repository for integrations with HuggingFace Transformers.
\end{abstract}


\section{Introduction}
% Current approaches have shown important/remarkable performance in blah blah blah blah (cite, cite, cite, cite). But, current approaches work in way X, which have weakness Y, which precludes them from doing Z. Recent work has allieviated weakness Y in ways A (cite), A'(cite), A''(cite), but we propose/introduce new method B

\section{Background}

% Originally, Resampled Importance Sampling was introduced by Talbot (cite) to solve the rendering equation where:

%L(y, w) = integral_A p(y, yx <-> w) * L_e(x->y)G(x<->y)V(x<->y)dA_x
% where radiance is L, point is y, direction w is an integral over all light emitting surfaces A
% for BSDF p, emitted radiance L_e, mutual visibility V between x and y, and a geometry term G

% simplify as L = integral_A f(x) dx

% Importance sampling
% Importance sampling estimates an integral by choosing N samples x_i from a source PDF p(x_i) and computing

% <L>_is^N = 1/N bigSum_i=1^N f(x_i)/p(x_i) ~= L
% IS reminds unbaised if p(x) is positive whenever f(x) is non-zero, and ideally p(x) is correlated with f(x) to reduce variance.
% The idea here is that we get to pick p(x), and if p(x) ~= f(x), then variance will be lowered as we won't waste any samples
% where f(x) was already low. Picking p(x) == f(x) is very hard because that requires us to know exactly what f(x) is already, and is
% known as 'perfect' importance sampling.

% Multiple importance sampling (MIS)

% Because picking proportion to f(x) is very hard, we want something easier. Instead, we can pick samples proportional to individual terms
% in the integrand (BSDF p or emissive surfaces L_e). Given M candidate strategies p_s, MIS does

% <L>_mis^M, N = bigsum_s=1^M 1/N_s bigsum_i=1^N_s w_s(x_i) f(x_i)/p_s(x_i)

% as long as the sum of the weights equals 1, MIS remains unbiased. The balance heuristic takes a weighted average of the strategies
% and is provably good.

%

\subsection{Resampled Importance Sampling (RIS)}

% Multiple importance sampling works well when you have a small number of effective strategies, but because the computational cost grows linearly with the # of strategies it becomes computationally infeasible when you're trying to sample millions of potential strategies
\section{Methodology}
\section{Evaluation}
\section{Related Work}
\section{Limitations}
\section{Conclusion}


\begin{ack}
\end{ack}

\section*{References}


\medskip


{
\small


[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.


[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.


[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Technical Appendices and Supplementary Material}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\end{document}